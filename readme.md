# Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction

This replication kit contains the jupyter notebooks used to aggregate the results of the mining process.
We describe all steps needed for a reproduction. However, due to the nature and size of the data not everything can be contained in this replication kit.


## SmartSHARK Data

The raw data used in this study was collected with the [SmartSHARK infrastructure](https://smartshark.github.io) over a multi-year time period. For every commit and every file static source code metrics and static analysis warnings were collected amongst other features.
A dump of the resulting MongoDB can be found [here](HANDLE).
The SmartSHARK documentation has the information needed to create this dataset, however a production deployment with connection to a HPC-Cluster is advisable due to the computation time required.


## Fine-grained just-in-time defect prediction data mining

The SmartSHARK database contains a snapshot of the Git repositories of the projects at the time the data was mined via SmartSHARK in GridFS.
After extracting the data from the GridFS to a local path the extraction can be started.

For fine-grained just-in-time defect prediction we accumulate a lot of data per file over its full history in the commit graph. Therefore a special tool is needed to extract this information in the right way.
The tool can be found [here](https://github.com/atrautsch/Gierlappen). It is a constantly changing research prototype that uses breadth-first-search, path slicing and a stack per path to traverse every possible path in a commit graph while keeping states for each path.

This tool was used to create the raw mined data that can be downloaded separately.


## Download data

This download contains the raw mined data used to generate the evaluation data already contained in this replication kit. You do not need to download this if you only want to re-create the plots and tables.
```bash
cd data
wget https://user.informatik.uni-goettingen.de/~trautsch2/icsme2020_mining_data.zip
unzip icsme2020_mining_data.zip
```

## Install dependencies for jupyter lab

```bash
python -m venv .
source bin/activate
pip install -r requirements.txt
```

## Run jupyter lab

```bash
source bin/activate
cd notebooks
jupyter lab
```

Jupyter lab is used to aggregate the results and create the plots.
To aggregate the raw mining data run the TrainTest.ipynb and Interval.ipynb notebooks.
Both aggregate the jit_sn_*.csv to aggregated results from the model evaluation.

The plots are generated by TrainTestEval.ipynb and IntervalEval.ipynb.

Overview.ipynb and Correlation.ipynb provide an overview of the data and the top 10 features.
