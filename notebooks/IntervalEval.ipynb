{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "PROJECTS = ['ant-ivy', 'archiva', 'calcite', 'cayenne', 'commons-bcel', 'commons-beanutils',\n",
    "            'commons-codec', 'commons-collections', 'commons-compress', 'commons-configuration',\n",
    "            'commons-dbcp', 'commons-digester', 'commons-io', 'commons-jcs', 'commons-jexl',\n",
    "            'commons-lang', 'commons-math', 'commons-net', 'commons-scxml', \n",
    "            'commons-validator', 'commons-vfs', 'deltaspike', 'eagle', 'giraph', 'gora', 'jspwiki',\n",
    "            'knox', 'kylin', 'lens', 'mahout', 'manifoldcf','nutch','opennlp','parquet-mr',\n",
    "            'santuario-java', 'systemml', 'tika', 'wss4j']\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "FIGURES_PATH = '../figures/'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load interval data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for project_name in PROJECTS:\n",
    "    fname = '{}/interval_mean_{}.csv'.format(DATA_PATH, project_name)\n",
    "    if os.path.exists(fname):\n",
    "        try:\n",
    "            scores.append(pd.read_csv(fname))\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(fname, 'is empty')\n",
    "    else:\n",
    "        print('missing', project_name)\n",
    "\n",
    "r = pd.concat(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression interval plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_name, lbl_name in zip(['bug', 'ad-hoc'], ['label_bug', 'label_adhoc']):\n",
    "    for pn1, ms1 in [('F-measure', 'lr_f1'), ('AUC', 'lr_roc_auc')]:\n",
    "        fig = plt.figure(figsize=(5,2))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.title('Logistic Regression')\n",
    "        bp = []\n",
    "        names = []\n",
    "        for pn, ms in [('combined', 'jit_static_pmd'), ('jit', 'jit'), ('static', 'static'), ('pmd', 'pmd')]:\n",
    "            bp.append(r[(r['metric_set'] == ms) & (r['label'] == lbl_name)][ms1].values)\n",
    "            names.append(pn)\n",
    "        ax.boxplot(bp)\n",
    "        for i, (name, bp) in enumerate(zip(names, bp)):\n",
    "            y = bp\n",
    "            x = np.random.normal(1+i, 0.05, size=len(y))\n",
    "            ax.plot(x, y, 'k.', alpha=0.05)\n",
    "        ax.set_xticklabels(names)\n",
    "        ax.set_ylabel(pn1)\n",
    "        ax.set_ylim([0,1])\n",
    "        plt.show()\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(FIGURES_PATH + '/interval_mean_{}_{}.pdf'.format(ms1, lbl_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest interval plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_name, lbl_name in zip(['bug', 'ad-hoc'], ['label_bug', 'label_adhoc']):\n",
    "    for pn1, ms1 in [('F-measure', 'rf_f1'), ('AUC', 'rf_roc_auc')]:\n",
    "        fig = plt.figure(figsize=(5,2))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.title('Random Forest')\n",
    "        bp = []\n",
    "        names = []\n",
    "        for pn, ms in [('combined', 'jit_static_pmd'), ('jit', 'jit'), ('static', 'static'), ('pmd', 'pmd')]:\n",
    "            bp.append(r[(r['metric_set'] == ms) & (r['label'] == lbl_name)][ms1].values)\n",
    "            names.append(pn)\n",
    "        ax.boxplot(bp)\n",
    "        for i, (name, bp) in enumerate(zip(names, bp)):\n",
    "            y = bp\n",
    "            x = np.random.normal(1+i, 0.05, size=len(y))\n",
    "            ax.plot(x, y, 'k.', alpha=0.05)\n",
    "        ax.set_xticklabels(names)\n",
    "        ax.set_ylabel(pn1)\n",
    "        ax.set_ylim([0,1])\n",
    "        plt.show()\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(FIGURES_PATH + '/interval_mean_{}_{}.pdf'.format(ms1, lbl_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = []\n",
    "for project in r['project'].unique():\n",
    "    for plot_name, lbl_name in zip(['bug', 'adhoc'], ['label_bug', 'label_adhoc']):\n",
    "        df = r[(r['label'] == lbl_name) & (r['metric_set'] != 'jit_statc') & (r['project'] == project)].copy()\n",
    "        for metric_set in df['metric_set'].unique():\n",
    "            tmp = df[df['metric_set'] == metric_set]\n",
    "            for i, _ in enumerate(tmp['rf_ub'].values):\n",
    "                dats.append({'classifier': 'Random Forest', 'project': project, 'feature_set': metric_set, 'ub': tmp['rf_ub'].values[i], 'lb': tmp['rf_lb'].values[i], 'label': plot_name})\n",
    "                dats.append({'classifier': 'Logistic Regression', 'project': project, 'feature_set': metric_set, 'ub': tmp['lr_ub'].values[i], 'lb': tmp['lr_lb'].values[i], 'label': plot_name})\n",
    "\n",
    "test = pd.DataFrame(dats)\n",
    "\n",
    "print('projects', test['project'].nunique())\n",
    "for label in test['label'].unique():\n",
    "    for ms in test['feature_set'].unique():\n",
    "        if ms not in ['jit', 'static', 'pmd', 'jit_static_pmd']:\n",
    "            continue\n",
    "        for cl in test['classifier'].unique():\n",
    "            tmp = test[(test['label'] == label) & (test['classifier'] == cl) & (test['feature_set'] == ms) & (np.isfinite(test['ub'])) & (np.isfinite(test['lb'])) & (test['ub'] > test['lb'])]\n",
    "            s = np.sum(tmp['ub'] - tmp['lb'])\n",
    "            print(label, ms, cl, len(tmp), '/', len(test[(test['label'] == label) & (test['classifier'] == cl) & (test['feature_set'] == ms)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dats = []\n",
    "for project in r['project'].unique():\n",
    "    for plot_name, lbl_name in zip(['bug', 'adhoc'], ['label_bug', 'label_adhoc']):\n",
    "        df = r[(r['label'] == lbl_name) & (r['metric_set'] != 'jit_statc') & (r['project'] == project)].copy()\n",
    "        for metric_set in df['metric_set'].unique():\n",
    "            tmp = df[df['metric_set'] == metric_set]\n",
    "            for i, _ in enumerate(tmp['rf_ub'].values):\n",
    "                dats.append({'classifier': 'Random Forest', 'project': project, 'feature_set': metric_set, 'ub': tmp['rf_ub'].values[i], 'lb': tmp['rf_lb'].values[i], 'label': plot_name})\n",
    "                dats.append({'classifier': 'Logistic Regression', 'project': project, 'feature_set': metric_set, 'ub': tmp['lr_ub'].values[i], 'lb': tmp['lr_lb'].values[i], 'label': plot_name})\n",
    "\n",
    "test = pd.DataFrame(dats)\n",
    "\n",
    "print('projects', test['project'].nunique())\n",
    "for label in test['label'].unique():\n",
    "    for ms in test['feature_set'].unique():\n",
    "        if ms not in ['jit', 'static', 'pmd', 'jit_static_pmd']:\n",
    "            continue\n",
    "        tmp = test[(test['label'] == label) & (test['feature_set'] == ms) & (np.isfinite(test['ub'])) & (np.isfinite(test['lb'])) & (test['ub'] > test['lb'])]\n",
    "        alls = test[(test['label'] == label) & (test['feature_set'] == ms)]\n",
    "        s = np.sum(tmp['ub'] - tmp['lb'])\n",
    "        print(label, ms, len(tmp)/2, len(alls)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per project\n",
    "for project in test['project'].unique():\n",
    "    save = 0\n",
    "    no_save = 0\n",
    "    for label in test['label'].unique():\n",
    "        for ms in test['feature_set'].unique():\n",
    "            if ms not in ['jit', 'static', 'pmd', 'jit_static_pmd']:\n",
    "                continue\n",
    "            tmp = test[(test['project'] == project) & (test['label'] == label) & (test['feature_set'] == ms) & (np.isfinite(test['ub'])) & (np.isfinite(test['lb'])) & (test['ub'] > test['lb'])]\n",
    "            no = test[(test['project'] == project) & (test['label'] == label) & (test['feature_set'] == ms) & ((~np.isfinite(test['ub'])) | (~np.isfinite(test['lb'])) | (test['ub'] <= test['lb']))]\n",
    "            should = len(test[(test['project'] == project) & (test['label'] == label) & (test['feature_set'] == ms)])\n",
    "\n",
    "            s = np.sum(tmp['ub'] - tmp['lb'])\n",
    "            save += len(tmp)\n",
    "            no_save += len(no)\n",
    "            if len(tmp) > 0 and len(tmp) == should:\n",
    "                print('FULL!')\n",
    "            # print(label, ms, len(tmp)/2)\n",
    "    print(project, save, no_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
